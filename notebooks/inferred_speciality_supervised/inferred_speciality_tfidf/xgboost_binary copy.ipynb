{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached h5py-3.9.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached grpcio-1.56.2-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: rsa, pyasn1-modules, opt-einsum, MarkupSafe, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, werkzeug, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.2 h5py-3.9.0 keras-2.13.1 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-intel-2.13.0 werkzeug-2.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  7 23:55:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.90       Driver Version: 528.90       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A550... WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "| N/A   44C    P3    29W /  55W |    544MiB / 16384MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(X):\n",
    "    X_normalized = []\n",
    "    for i in range(len(X)):\n",
    "        X_normalized.append(np.array(X[i])/sum(X[i]))\n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(file_name, file_path):\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_name, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(npi_indigo_spl):\n",
    "    npi_indigo_spl_non_nan = {}\n",
    "    for npi in npi_indigo_spl:\n",
    "        if isinstance(npi_indigo_spl[npi], str):\n",
    "            npi_indigo_spl_non_nan[npi] = npi_indigo_spl[npi]\n",
    "    return npi_indigo_spl_non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(npi_distributions, npi_indigo_spl):\n",
    "    dataset = {'NPI':[], 'features':[], 'labels':[]}\n",
    "    for npi in npi_distributions:\n",
    "        if int(npi) in npi_indigo_spl:\n",
    "            dataset['NPI'].append(npi)\n",
    "            dataset['features'].append(npi_distributions[npi])\n",
    "            dataset['labels'].append(npi_indigo_spl[int(npi)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_less_than_threshold_categories(y, threshold):\n",
    "    labels_count = {}\n",
    "    for lab in y:\n",
    "        if lab in labels_count:\n",
    "            labels_count[lab] += 1\n",
    "        else:\n",
    "            labels_count[lab] = 1\n",
    "\n",
    "    useless = []\n",
    "    for lab in labels_count:\n",
    "        if labels_count[lab] < threshold:\n",
    "            useless.append(lab)\n",
    "    return useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless(useless, y, X):\n",
    "    for i in reversed(range(len(y))):\n",
    "        if y[i] in useless:\n",
    "            del y[i]\n",
    "            del X[i]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def encode_labels(y):\n",
    "#    label_encoder = LabelEncoder()\n",
    "#    return label_encoder.fit_transform(y), label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(y_train):\n",
    "    check = set(y_train)\n",
    "\n",
    "    print('Max value in input: ' +  str(max(check)))\n",
    "    print('\"length-1\" of set of input: ' + str(len(check)-1))\n",
    "    assert max(check) == (len(check)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_y(X_train_, y_train_, remove_list):\n",
    "    \n",
    "    X_train_rm = []\n",
    "    y_train_rm = []\n",
    "\n",
    "    for i in range(len(X_train_)):\n",
    "        if y_train_[i] not in remove_list:\n",
    "            X_train_rm.append(X_train_[i])\n",
    "            y_train_rm.append(y_train_[i])\n",
    "    return X_train_rm, y_train_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_distribution(y_train, k):\n",
    "    dataset_distribution = {}\n",
    "    total = 0\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] not in dataset_distribution:\n",
    "            dataset_distribution[y_train[i]] = 1\n",
    "        else:\n",
    "            dataset_distribution[y_train[i]] += 1\n",
    "        total += 1\n",
    "    \n",
    "    print(dataset_distribution)\n",
    "    remove_list = []\n",
    "    for key in dataset_distribution:\n",
    "        if dataset_distribution[key] < k:\n",
    "            remove_list.append(key)\n",
    "    \n",
    "    print(list(dataset_distribution.values()))\n",
    "    print(max(list(dataset_distribution.values())))\n",
    "    print(min(list(dataset_distribution.values())))\n",
    "    print(np.std(list(dataset_distribution.values())))\n",
    "    return remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels_2way(y):\n",
    "    encoded_y = []\n",
    "    for l in y:\n",
    "        if '-Surgery' in l or '-Minor' in l:\n",
    "            encoded_y.append(0)\n",
    "        else:\n",
    "            encoded_y.append(1)\n",
    "    return encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels_2way_custom(y, label):\n",
    "    encoded_y = []\n",
    "    for l in y:\n",
    "        if label in l:\n",
    "            encoded_y.append(0)\n",
    "        else:\n",
    "            encoded_y.append(1)\n",
    "    return encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overpopulate(X, y, op_multiplier, op_ratio_retained):\n",
    "    X_oped = []\n",
    "    y_oped = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            for j in range(op_multiplier):\n",
    "                X_oped.append(X[i])\n",
    "                y_oped.append(y[i])\n",
    "        elif random.uniform(0, 1) < op_ratio_retained:\n",
    "            X_oped.append(X[i])\n",
    "            y_oped.append(y[i])\n",
    "    \n",
    "    return X_oped, y_oped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_cpts_with_multipliers(normalized_distributions, k):\n",
    "    \n",
    "    all_cpt_pos = {}\n",
    "    for i in range(len(normalized_distributions)):\n",
    "        cur_top_agg = normalized_distributions[i].argsort()[-k:][::-1]\n",
    "        for pos in cur_top_agg:\n",
    "            if pos not in all_cpt_pos:\n",
    "                all_cpt_pos[pos] = 0\n",
    "            all_cpt_pos[pos] += 1\n",
    "                \n",
    "    for cpt_pos in all_cpt_pos:\n",
    "        all_cpt_pos[cpt_pos] = np.log(len(normalized_distributions)/all_cpt_pos[cpt_pos])\n",
    "    \n",
    "    return all_cpt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_cpts(normalized_distributions, k):\n",
    "    \n",
    "    all_cpt_pos = {}\n",
    "    for i in range(len(normalized_distributions)):\n",
    "        cur_top_agg = normalized_distributions[i].argsort()[-k:][::-1]\n",
    "        for pos in cur_top_agg:\n",
    "            if pos not in all_cpt_pos:\n",
    "                all_cpt_pos[pos] = 0\n",
    "            all_cpt_pos[pos] += 1\n",
    "                \n",
    "    for cpt_pos in all_cpt_pos:\n",
    "        all_cpt_pos[cpt_pos] = np.log(len(normalized_distributions)/all_cpt_pos[cpt_pos])\n",
    "    \n",
    "    return all_cpt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vector(all_cpt_pos, num_featurs):\n",
    "    idf_vector = []\n",
    "    for i in range(num_featurs):\n",
    "        if i in all_cpt_pos:\n",
    "            idf_vector.append(all_cpt_pos[i])\n",
    "        else:\n",
    "            idf_vector.append(0)\n",
    "\n",
    "    return idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i]/sum(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distributions(X, y):\n",
    "    distributions = [np.array(len(X[0])*[0.0]) for _ in range(len(set(y)))]\n",
    "    for i in range(len(X)):\n",
    "        if i%10000 == 0:\n",
    "            print(\"Done: \" + str(i))\n",
    "        distributions[y[i]] += X[i]\n",
    "    \n",
    "    distributions[1] += distributions[0]\n",
    "    \n",
    "    distributions = normalize(distributions)\n",
    "\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(tf_matrix, idf_vector):\n",
    "    tfidfs = []\n",
    "    for tf in tf_matrix:\n",
    "        tfidfs.append(np.multiply(tf, idf_vector[0]))\n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimention(all_cpt_idfs, X_normalized):\n",
    "    cpt_pos = list(all_cpt_idfs.keys())\n",
    "    print(cpt_pos)\n",
    "\n",
    "    X_reduced = []\n",
    "\n",
    "    for distribution in X_normalized:\n",
    "        temp = []\n",
    "        for pos in cpt_pos:\n",
    "            if all_cpt_idfs[pos] >= 0.001:\n",
    "                temp.append(distribution[pos])\n",
    "        X_reduced.append(temp)\n",
    "    \n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "npi_features = pd.read_pickle('./chuncked_npi_ncpcs_2019_0_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "npi_indigo_spl = pd.read_pickle('./npi_indigo_spl.pkl')\n",
    "npi_indigo_spl_non_nan = remove_nan(npi_indigo_spl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(npi_features, npi_indigo_spl_non_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['features']\n",
    "y = dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153954\n",
      "153954\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Maybe try a binary model of seperating surgical and non-surgical doctors and then create further indigo speciality distinctions for each of these 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Create a binary model each indigo speciality (True / False). Kind of a multi-label classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(random_search, results_test, results_train, X_val, y_val, X_train, y_train, spl):\n",
    "    predicted_probabilities = random_search.best_estimator_.predict_proba(X_val)\n",
    "    predictions_test = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "    predicted_probabilities = random_search.best_estimator_.predict_proba(X_train)\n",
    "    predictions_train = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "    test_out = classification_report(y_val, predictions_test, target_names=[spl, 'Others'], output_dict=True)\n",
    "    train_out = classification_report(y_train, predictions_train, target_names=[spl, 'Others'], output_dict=True)\n",
    "\n",
    "    results_train.append([\"CPT upper level features\", \"Train\", spl, train_out['Others']['precision'], train_out['Others']['recall'], train_out[spl]['precision'], train_out[spl]['recall'], train_out['macro avg']['f1-score']])\n",
    "    results_test.append([\"CPT upper level features\", \"Test\", spl, test_out['Others']['precision'], test_out['Others']['recall'], test_out[spl]['precision'], test_out[spl]['recall'], test_out['macro avg']['f1-score']])\n",
    "\n",
    "    return results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spit_train_test_split(X, y, speciality, param_dist, results_test, results_train, majority_class_data_split=0.1, minority_class_op_coef=1, test_size=0.3, random_state=897):\n",
    "    encoded_y = encode_labels_2way_custom(y, speciality)\n",
    "    encoded_y_np = np.array(encoded_y)\n",
    "\n",
    "    if len(encoded_y_np[encoded_y_np==0]) > 0:\n",
    "\n",
    "        X_oped, y_oped = overpopulate(X, encoded_y, minority_class_op_coef, majority_class_data_split)\n",
    "        X_oped_normalized = preprocessing_1(X_oped)\n",
    "\n",
    "        normalized_class_distributions = get_distributions(X_oped_normalized, y_oped)\n",
    "        all_cpt_pos_idf = get_top_k_cpts(normalized_class_distributions, 25)\n",
    "        idf_vector = get_idf_vector(all_cpt_pos_idf, len(X_oped_normalized[0]))\n",
    "\n",
    "        X_oped_normalized_reduced = reduce_dimention(all_cpt_pos_idf, X_oped_normalized)\n",
    "        # print(np.array(X_oped_normalized_reduced).shape)\n",
    "        idf_vector_reduced = reduce_dimention(all_cpt_pos_idf, [idf_vector])\n",
    "\n",
    "        #print(idf_vector_reduced)\n",
    "\n",
    "        X_oped_normalized_reduced_tfidfs = get_tfidf(X_oped_normalized_reduced, idf_vector_reduced)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_oped_normalized_reduced_tfidfs, y_oped, test_size=test_size, random_state=random_state, stratify=y_oped, shuffle=True)\n",
    "\n",
    "        model = xgb.XGBClassifier(objective='multi:softprob', num_class=len(set(y_train)), tree_method='gpu_hist', gpu_id=0)\n",
    "        random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=4, scoring='accuracy', n_jobs=-1, cv=2, verbose=3, random_state=53)\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        results_train, results_test = get_results(random_search, results_test, results_train, X_val, y_val, X_train, y_train, speciality)\n",
    "        print('\\033[1m' + '\\033[92m' + \"Done: \" + speciality + '\\033[0m' + '\\033[0m')\n",
    "    else:\n",
    "        print('\\033[1m' + '\\033[91m' + speciality + \" not in CPT upper level features\" + '\\033[0m' + '\\033[0m')\n",
    "    \n",
    "    return results_train, results_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specialities_lis = pd.read_pickle(\"./specialities_lis.pkl\")\n",
    "specialities_lis = [\"Cardiovascular Disease-Minor Surgery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0\n",
      "Done: 10000\n",
      "[15706, 14462, 15704, 15705, 15714, 15713, 14461, 15712, 15703, 2450, 15762, 2451, 12086, 407, 14457, 7394, 14463, 15755, 15733, 13929, 3837, 15715, 10815, 8934, 15753, 6073, 239, 238, 6075, 6080, 14456, 12084, 6074, 236, 14460, 7393, 12571, 13927, 189, 191, 14896, 190]\n",
      "[15706, 14462, 15704, 15705, 15714, 15713, 14461, 15712, 15703, 2450, 15762, 2451, 12086, 407, 14457, 7394, 14463, 15755, 15733, 13929, 3837, 15715, 10815, 8934, 15753, 6073, 239, 238, 6075, 6080, 14456, 12084, 6074, 236, 14460, 7393, 12571, 13927, 189, 191, 14896, 190]\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganga\\anaconda3\\envs\\inferred_speciality\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mDone: Cardiovascular Disease-Minor Surgery\u001b[0m\u001b[0m\n",
      "Done:  Cardiovascular Disease-Minor Surgery\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Type</td>\n",
       "      <td>Train / Test</td>\n",
       "      <td>Speciality</td>\n",
       "      <td>Precision 0</td>\n",
       "      <td>Recall 0</td>\n",
       "      <td>Precision 1</td>\n",
       "      <td>Recall 1</td>\n",
       "      <td>Macro Avg F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPT upper level features</td>\n",
       "      <td>Test</td>\n",
       "      <td>Cardiovascular Disease-Minor Surgery</td>\n",
       "      <td>0.987527</td>\n",
       "      <td>0.992865</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.85065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Type</td>\n",
       "      <td>Train / Test</td>\n",
       "      <td>Speciality</td>\n",
       "      <td>Precision 0</td>\n",
       "      <td>Recall 0</td>\n",
       "      <td>Precision 1</td>\n",
       "      <td>Recall 1</td>\n",
       "      <td>Macro Avg F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPT upper level features</td>\n",
       "      <td>Train</td>\n",
       "      <td>Cardiovascular Disease-Minor Surgery</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951899</td>\n",
       "      <td>0.987239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0             1  \\\n",
       "0                 Data Type  Train / Test   \n",
       "1  CPT upper level features          Test   \n",
       "2                 Data Type  Train / Test   \n",
       "3  CPT upper level features         Train   \n",
       "\n",
       "                                      2            3         4            5  \\\n",
       "0                            Speciality  Precision 0  Recall 0  Precision 1   \n",
       "1  Cardiovascular Disease-Minor Surgery     0.987527  0.992865     0.772414   \n",
       "2                            Speciality  Precision 0  Recall 0  Precision 1   \n",
       "3  Cardiovascular Disease-Minor Surgery     0.998242       1.0          1.0   \n",
       "\n",
       "          6             7  \n",
       "0  Recall 1  Macro Avg F1  \n",
       "1  0.658824       0.85065  \n",
       "2  Recall 1  Macro Avg F1  \n",
       "3  0.951899      0.987239  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 300, 50),\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [6, 7, 8, 9],\n",
    "    'colsample_bytree': [0.6, 0.65, 0.7, 0.75],\n",
    "}\n",
    "\n",
    "results_test = [[\"Data Type\", \"Train / Test\", \"Speciality\", \"Precision 0\", \"Recall 0\", \"Precision 1\", \"Recall 1\", \"Macro Avg F1\"]]\n",
    "results_train = [[\"Data Type\", \"Train / Test\", \"Speciality\", \"Precision 0\", \"Recall 0\", \"Precision 1\", \"Recall 1\", \"Macro Avg F1\"]]\n",
    "for spl in specialities_lis:\n",
    "    results_train, results_test = spit_train_test_split(X, y, spl, param_dist, results_test, results_train)\n",
    "    print(\"Done: \", spl)\n",
    "    break\n",
    "\n",
    "df = pd.DataFrame(results_test + results_train)\n",
    "# df\n",
    "df.to_csv('./binary_spl_prediction_tfidf_top25_output_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inferred_speciality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
